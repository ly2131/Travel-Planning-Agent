from scrapy.crawler import CrawlerProcess
from price_spider.price_spider.spiders.city_detail import CityDetailSpider
import os
import json


def run_sequential(city_list):
    output_file = "result.json"

    # åˆ é™¤æ—§æ–‡ä»¶ï¼Œé¿å…å†…å®¹å†²çª
    if os.path.exists(output_file):
        os.remove(output_file)

    # åˆå§‹åŒ– Scrapy CrawlerProcess
    process = CrawlerProcess(settings={
        "FEEDS": {
            output_file: {
                "format": "jsonlines",  # æ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡
                "encoding": "utf8",
            },
        },
        "DEFAULT_REQUEST_HEADERS": {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/115.0.0.0 Safari/537.36",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Referer": "https://www.google.com/",
        }
    })

    for city in city_list:
        print(f"\nğŸŒ æ­£åœ¨çˆ¬å–åŸå¸‚: {city}")
        process.crawl(CityDetailSpider, city=city)

    process.start()

    return load_jsonlines(output_file)


def load_jsonlines(filepath: str):
    data = []
    with open(filepath, "r", encoding="utf8") as f:
        for i, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                data.append(json.loads(line))
            except json.JSONDecodeError as e:
                print(f"âŒ JSON è§£æå¤±è´¥ (line {i}): {e}")
                print(f"ğŸ” å†…å®¹: {repr(line)}")
    return data


# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    cities = ["beijing", "bangkok", "paris"]
    result = run_sequential(cities)

    print("\nâœ… æ‰€æœ‰åŸå¸‚çˆ¬å–å®Œæˆï¼Œå…±", len(result), "æ¡æ•°æ®")
    for item in result:
        print(f"\nğŸŒ åŸå¸‚: {item.get('city', ['Unknown'])[0]}")
        print(json.dumps(item, indent=2, ensure_ascii=False))
